---
title: ""
format: 
  revealjs:
    theme: [default, custom.scss]
    footer: '<span style="color:#bf7971;">github.com/simonpcouch/conf-25</span>'
editor: source
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
revealjs-plugins:
  - editable
filters:
  - editable
---

# Is that LLM feature any good?


:::::: {.columns}
::: {.column width="70%"}
<br><br><br><br><br>
Simon P. Couch - Posit PBC
:::
::: {.column width="30%"}
![](figures/bear/happy-bear.png){width="250px"}
:::
::: 


##  {.nostretch}

![](figures/conf-bot-post.png){style="display: block; margin: 0 auto; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);" width="65%"}

## 


![](figures/conf-bot.png){.absolute width=965.695px height=650.862px left=41px top=19.2754px}

:::notes
When explaining RAG:
* Describe it in plain terms, e.g. you have a bunch of text and you chunk it up so that the LLM can search through it before responding to your question.
* Note that "RAG" is short for "retrieval augmented generation" since "RAG" shows up in slides later.
:::

##

![](figures/conf-bot.png){.absolute width=965.695px height=650.862px left=41px top=19.2754px}

![](figures/bear/narcissistic-bear.png){.absolute width=411px left=665px top=-35px}

## 

![](figures/conf-bot-narcissus.png){.absolute width=965.695px height=650.862px left=41px top=19.2754px}

![](figures/bear/narcissistic-bear.png){.absolute width=411px left=665px top=-35px}

## 

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("figures/conf-bot-response.png")
```

. . .

![](figures/bear/teary-eyed-bear.png){.absolute width=150.101px left=0px top=517.115px}

##

<br>

![](figures/vitals-intro/vitals-intro.001.png){fig-align="center"}

##

<br>

![](figures/vitals-intro/vitals-intro.002.png){fig-align="center"}

##

<br>

![](figures/vitals-intro/vitals-intro.003.png){fig-align="center"}

## When it comes to running evals on your LLM apps...

. . .

::: {style="display: flex; flex-direction: column; align-items: center; justify-content: center; gap: 30px; margin-top: 20px; width: 100%; height: 50vh;"}
::: {style="background-color: #E8A098; padding: 60px 60px; border-radius: 20px; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15); text-align: center; width: 90%; display: flex; align-items: center; justify-content: center; flex: 1;"}
<h3 style="color: white; margin: 0;">You really should be!</h3>
:::

::: {style="background-color: #5B7FA0; padding: 60px 60px; border-radius: 20px; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15); text-align: center; width: 90%; display: flex; align-items: center; justify-content: center; flex: 1;"}
<h3 style="color: white; margin: 0;">You can do it!</h3>
:::
:::

# [You really should be!]{style="color: white;"} {background-color="#E8A098"}

[[evaluating your LLM product]]{style="color: white;"}

##

<br><br><br><br>

> _Like software engineering, success with AI hinges on how fast you can iterate._ - Hamel Husain

:::footer
<span style="color:#bf7971;">https://hamel.dev/blog/posts/evals/</span>
:::

## 

In SWE:

:::::: {.columns}
::: {.column width="40%"}
1) Make changes
2) Evaluate quality
3) Debug issues
:::
::: {.column width="60%"}
(coding)

(unit testing)
:::
:::

## 

In AI:

:::::: {.columns}
::: {.column width="33%"}
1) Make changes
:::
::: {.column width="67%"}
(prompt engineering, fine-tuning, ...)
:::
:::

##

In AI:

:::::: {.columns}
::: {.column width="33%"}
1) Make changes
2) Have a looksie
3) Whack-a-mole 
:::
::: {.column width="67%"}
(prompt engineering, fine-tuning, ...)
:::
:::

![](figures/bear/whack-a-mole.png){.absolute width=390.5px height=220px left=365px top=150px}

:::notes
Tie the whack-a-mole to the real risk here--if you can't resolve issues without introducing others, you will struggle to take your app from demo to production.
:::

## 

AI ü§ù SWE tooling

:::::: {.columns}
::: {.column width="33%"}
1) Make changes
2) Evaluate quality
3) Debug issues 
:::
::: {.column width="67%"}
(prompt engineering, fine-tuning, ...)
:::
:::

![](figures/hexes/vitals.png){.absolute width=204.439px left=355px top=150px}

:::notes
If there's enough time here, this is the "why not just use an off-the-shelf eval?"
- ‚Äòthis new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.‚Äô [Source](https://thezvi.substack.com/p/ai-130-talking-past-the-sale)
:::

# You can do it! {background-color="#5B7FA0"}

## 

Evaluations of ellmer-based apps are plug-and-play:

![](figures/bear/plug-and-play.png){fig-align="center"}

## 

Evaluations of ellmer-based apps are plug-and-play:

![](figures/bear/plug-and-play-2.png){fig-align="center"}

## 

<br><br>

Evaluations have three pieces:

* Dataset
* Solver
* Scorer

::: aside
::: {.progress-tracker}
::: {.progress-item}
Dataset
:::
::: {.progress-item}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .active}
Dataset
:::
::: {.progress-item}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

##

<br><br>

A few:

* `input`s: Prompts that users might provide
* `target`s: Corresponding grading guidance

. . .

e.g.

* `input`: "Are there any talks about evals?"
* `target`: "Yes, Simon Couch will be giving a talk called 'Is that LLM feature any good?'"

::: aside
::: {.progress-tracker}
::: {.progress-item .active}
Dataset
:::
::: {.progress-item}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

## 

<br>

![](figures/spreadsheet.png){fig-align="center" height=600px}

::: aside
::: {.progress-tracker}
::: {.progress-item .active}
Dataset
:::
::: {.progress-item}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::


## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .active}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

##

<br><br>

The conf chat app looks like this:

```{r}
#| label: client
#| eval: false
#| code-line-numbers: "|3|5|7|"
library(ellmer)

client <- chat_openai()

# add prompting, a RAG tool, etc...

live_browser(client)
```

. . .

Your solver is the `client`‚úÖ

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .active}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item}
Scorer
:::
:::
:::

## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item .active}
Scorer
:::
:::
:::

## 

<br><br>

```{r}
#| label: interpolate
#| eval: false
client$chat(glue::glue("
  You are assessing a submitted answer on a given task 
  based on a criterion.

  [Task]: {input}

  [Submission]: {solver_response}

  [Criterion]: {target}

  Does the submission meet the criterion?
"))
```

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item .active}
Scorer
:::
:::
:::

## 

<br><br>

_Wait, we're using an LLM to grade an LLM's output?_

. . .

![](figures/bear/spiderman.png){.absolute width=410.289px left=500.711px top=270.892px}

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item .active}
Scorer
:::
:::
:::

## 

<br><br>

_Wait, we're using an LLM to grade an LLM's output?_

![](figures/bear/spiderman-happy.png){.absolute width=410.289px left=500.711px top=270.892px}

<br><br><br><br><br><br><br>
Yes.


:::notes
Don't take my word for it, though. 

As much as an evaluation toolkit is about evaluating quality, it is about debugging issues.
:::

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item .active}
Scorer
:::
:::
:::

## 

<br><br>

::: aside
::: {.progress-tracker}
::: {.progress-item .completed}
Dataset
:::
::: {.progress-item .completed}
Solver
:::
::: {.progress-item .completed}
Scorer
:::
:::
:::

##

```{r}
#| label: run-eval
#| include: false
#| eval: false
devtools::load_all()
client <- conf_client()

Sys.setenv(VITALS_LOG_DIR = "inst/logs/")

spreadsheet <- "https://docs.google.com/spreadsheets/d/1Pm0_itdB61H539zf8Ksm8l_6r-8tqspuvZUdFVoLwKg/edit?usp=sharing"

spreadsheet <- googlesheets4::read_sheet(spreadsheet)

usethis::use_data(spreadsheet, overwrite = TRUE)

tsk <- Task$new(
  dataset = spreadsheet,
  solver = generate(client),
  scorer = model_graded_qa(
    scorer_chat = ellmer::chat_openai(model = "gpt-4.1"),
    instructions = 
      paste0(
        "Details provided in the target response are a superset of the information that forms a correct response. For example, if a input asks 'when' something happens, an answer that notes 'when' some event is without noting other details‚Äîlike the location or speakers‚Äîis correct.",
        vitals:::qa_default_instructions()
      )
    )
)

tsk$eval()

vitals::vitals_bundle("inst/logs/", "inst/log-bundle")
```

```{r}
#| label: define-eval
#| message: false
#| warning: false
#| eval: false
#| code-line-numbers: "|3|5-8|"
library(vitals)

spreadsheet <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1Pm0_itdB61H539zf8Ksm8l_6r-8tqspuvZUdFVoLwKg/edit?usp=sharing")

tsk <- Task$new(
  dataset = spreadsheet,
  solver = generate(client),
  scorer = model_graded_qa()
)
```

:::notes
TODO: should we move the `library(vitals)` up to an earlier slide?
:::

##

```{r}
#| label: mock-eval
#| eval: false
tsk$eval()
```

. . . 

```
‚úÖ Solving [7s]                                                    
‚úÖ Scoring [5.5s]
```

. . .

```{r}
#| label: log-bundle
#| echo: false
htmltools::tags$iframe(
  src = "inst/log-bundle/index.html#/logs/2025-09-09T15-13-25-05-00_spreadsheet_6feda01840c754f620e28f.json",
  width = "100%", 
  height = "550px",
  style = "border-radius: 10px; box-shadow: 0 5px 10px rgba(0, 0, 0, 0.3);"
) 
```

## When it comes to running evals on your LLM apps...

::: {style="display: flex; flex-direction: column; align-items: center; justify-content: center; gap: 30px; margin-top: 20px; width: 100%; height: 50vh;"}
::: {style="background-color: #E8A098; padding: 60px 60px; border-radius: 20px; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15); text-align: center; width: 90%; display: flex; align-items: center; justify-content: center; flex: 1;"}
<h3 style="color: white; margin: 0;">You really should be!</h3>
:::

::: {style="background-color: #5B7FA0; padding: 60px 60px; border-radius: 20px; box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15); text-align: center; width: 90%; display: flex; align-items: center; justify-content: center; flex: 1;"}
<h3 style="color: white; margin: 0;">You can do it!</h3>
:::
:::

##

![](figures/closing-chat.png)

![](figures/bear/happy-bear.png){.absolute width=135px height=165px left=730px top=358px}
