# Is that LLM feature any good? <a href="https://vitals.tidyverse.org/"><img src="figures/hexes/vitals.png" alt="The hex sticker for the vitals package: a teddy bear in blue scrubs happily holding a stethoscope." align="right" height="200"/></a>

Source code and slides for my talk "Is that LLM feature any good?" at [posit::conf(2025)](https://posit.co/conference/)

This is a talk about LLM evaluation with [Inspect](https://inspect.aisi.org.uk/) and [vitals](https://vitals.tidyverse.org/). vitals is an R port of Inspect, a widely-adopted framework for LLM evaluation.

Various nods:

* The example underlying this talk is Sam Parmar's awesome [posit::conf(2025) agenda chat app](https://github.com/parmsam/posit-conf-2025-chat).
* Thanks to Kristin Bott for her input on earlier versions of this talk. 
* The points on the link between success in SWE and pace of iteration draw from Hamel Husain's blog post [Your AI Product Needs Evals](https://hamel.dev/blog/posts/evals/). 
* The variants of the teddy bear in the stacks hex were generated with Gemini 2.5 Flash Image.
