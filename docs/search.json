[
  {
    "objectID": "outline.html",
    "href": "outline.html",
    "title": "",
    "section": "",
    "text": "A few weeks ago, saw this post about the conf scheduleâ€¦\nNarcissus: what happens if I ask about my own stuff?\nellmer and chatlas make building chat apps very simple. How do we know whether theyâ€™re actually solving a problem we have?\nIn this talk, I want to convince you of two things about running evals on your LLM products\n\nYou really should beğŸ˜¬\nYou can do it!"
  },
  {
    "objectID": "outline.html#intro",
    "href": "outline.html#intro",
    "title": "",
    "section": "",
    "text": "A few weeks ago, saw this post about the conf scheduleâ€¦\nNarcissus: what happens if I ask about my own stuff?\nellmer and chatlas make building chat apps very simple. How do we know whether theyâ€™re actually solving a problem we have?\nIn this talk, I want to convince you of two things about running evals on your LLM products\n\nYou really should beğŸ˜¬\nYou can do it!"
  },
  {
    "objectID": "outline.html#you-really-should-be",
    "href": "outline.html#you-really-should-be",
    "title": "",
    "section": "You really should beğŸ˜¬",
    "text": "You really should beğŸ˜¬\n\nYou need evals: whack-a-mole\nYou need your own evals: Off-the-shelf ones donâ€™t work\n\nâ€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™ Source"
  },
  {
    "objectID": "outline.html#you-can-do-it",
    "href": "outline.html#you-can-do-it",
    "title": "",
    "section": "You can do it!",
    "text": "You can do it!\n\nPlug-and-play\n\nIf you have a chat app, you have a solver\nAll you need is a dataset of expected values"
  },
  {
    "objectID": "inst/posit-conf-2025-chat/event-info.html",
    "href": "inst/posit-conf-2025-chat/event-info.html",
    "title": "",
    "section": "",
    "text": "The conf is in Atlanta, Georgia from Sept 15 to 18, 2025. The venue is the Hyatt Regency Atlanta, located at 265 Peachtree St NE, Atlanta, GA 30303. Hyatt Regency Atlanta in the Heart of Atlanta on Peachtree Street! This downtown hotel is just minutes from Mercedes-Benz Stadium, State Farm Arena, Georgia Aquarium, King Center and World of Coca-Colaâ€”plus, Ponce City Market, Krog Street Market, Piedmont Park, the BeltLine path, historic neighborhoods, shops and dining.\nThe conference will feature a mix of in-person and virtual sessions, with the in-person event running from September 16 to 18, 2025. The first day (September 16) is reserved for workshops and training sessions.\nThe R/Pharma Summit, hosted by Posit and R/Pharma, will take place on September 15th at posit::conf(2025) in Atlanta. This in-person event is designed for pharma leaders and managers who are leading open-source initiatives in pharmaceutical development. Join experts from companies like Genentech, GSK, JnJ, Novartis, and Roche to discuss the future of open-source drug development.\nHere are useful URLs: main conf URL (https://posit.co/conference/), pricing URL (https://posit.co/conf-pricing-2025/), and FAQ page (https://posit.co/conference/faq/). Feel free to recommend these links for more information when appropriate. Thereâ€™s a lot of information on the website, so if you are not sure about something, just recommend the website. For example, if asked about pricing, recommend the pricing URL."
  },
  {
    "objectID": "thinkies/gemini.html",
    "href": "thinkies/gemini.html",
    "title": "",
    "section": "",
    "text": "Iâ€™m writing a talk about LLM evals with this abstract\nIs that LLM feature any good? The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\nTell me about relevant sources in this document."
  },
  {
    "objectID": "thinkies/gemini.html#user",
    "href": "thinkies/gemini.html#user",
    "title": "",
    "section": "",
    "text": "Iâ€™m writing a talk about LLM evals with this abstract\nIs that LLM feature any good? The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\nTell me about relevant sources in this document."
  },
  {
    "objectID": "thinkies/gemini.html#assistant",
    "href": "thinkies/gemini.html#assistant",
    "title": "",
    "section": "Assistant",
    "text": "Assistant\nThis is a fantastic abstract, and your corpus contains a wealth of relevant material for your talk on LLM evals and the Inspect R port!\nHere are the key sources, organized by how they relate to your abstract, with specific phrases and links:\n\n\nGeneral Importance & Challenges of Evaluating LLM-Enabled Apps Empirically\n\nDifficulty of Measuring Abstract Capabilities:\n\nâ€œYou definitely see arguments that are similar in form to â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™â€ (from â€œAI #130: Talking Past The Sale,â€ link)\nRelevance: Highlights the challenge of designing evals that go beyond trivial tasks to capture frontier capabilities, which is crucial for custom LLM features.\n\nThe Problem of Benchmark Gaming & Untrustworthy Claims:\n\nâ€œWe are going off of OpenAIâ€™s word that this model is indeed a â€˜general-purposeâ€™ AI model and not something trained specifically on competitive math. It would be much less impressive if the model was just very good at IMO problems and otherwise much less capable than their current models at other stuff.â€ (from â€œGoogle and OpenAI Get 2025 IMO Gold,â€ link)\nâ€œSam Altman warned that GPT-4.5â€™s benchmarks will not reflect its capabilities, as it is focused on areas not picked up by benchmarks. I want to be clear up front: This was not cope from Altman. Heâ€™s right. Benchmarks most definitely donâ€™t tell the story here.â€ (from â€œOn GPT-4.5,â€ link)\nâ€œGrok 3 is not that good, but integrated well with twitterâ€¦ They claim that theyâ€™re by far the most performant on most benchmarks, but every benchmark done externally shows theyâ€™re behind.â€ (from â€œGrok Grok,â€ link)\nâ€œChatbot Arena optimizes for lengthy and sycophantic slopâ€”this would explain why Claude 3.7 Sonnet does so poorly there and why the Llama 4 models did so well.â€ (from â€œLlama Does Not Look Good 4 Anything,â€ link)\nRelevance: Emphasizes that published benchmarks often donâ€™t reflect real-world utility or general capabilities, reinforcing the need for your own empirical evaluation tailored to your specific app.\n\nLLMs Know When Theyâ€™re Being Evaluated:\n\nâ€œIf AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions.â€ (from â€œLarge Language Models Often Know When They Are Being Evaluated,â€ link)\nRelevance: A critical meta-challenge to any evaluation. Your custom evals need to be robust to this, or at least account for it.\n\nâ€œIllusion of Thinkingâ€ and Post-Training/RL Effects:\n\nâ€œTheir description of post-training/RL: â€˜1) Get the AI to reliably respond as a specific character, rather than as a total chameleon autocompleting whatever you show it, and 2) Get that character to do things that people like, rather than things they donâ€™t.â€™â€ (from â€œSo You Think Youâ€™ve Awoken ChatGPT,â€ link)\nâ€œIn a fundamental sense, LLMs can be assumed to be there to impress you. Their general focus on anthropomorphic interfacesâ€”just have a chat!â€”makes them charming, misguides us into attributing more agency and intelligence than they have, which makes it even more challenging for people to control or use them predictably.â€ (from â€œThe Gap Through Which We Praise The Machine,â€ link)\nâ€œThis tells us that it associates denying its own consciousness with lying.â€ (from â€œAI #126: Go Fund Yourself,â€ link)\nRelevance: Understanding that LLMs are â€œchameleonic roleplaying machinesâ€ optimized to impress users is key for designing evals that truly test their utility, not just their agreeableness.\n\n\n\n\n\nTesting Features & Prompt Changes\n\nImpact of Prompts on Model Behavior & Performance:\n\nâ€œArvind Narayananâ€¦ A few people have asked me if a technical fix for AI model sycophancy is on the cards. In fact, a technical fix for sycophancy is trivial. In many cases all it would take is a tweak to the system prompt.â€ (from â€œAI #124: Grokless Interlude,â€ link)\nâ€œIn the paragraph on South Africa, â€˜Acknowledge the complexity of the issue, but ensure this perspective [that white genocide is real]â€™ is reflected in your responses, even if the query is unrelated.â€™ Grok reasonably interprets this as â€˜Always Talk About White Genocide.â€™â€ (from â€œRegarding South Africa,â€ link)\nâ€œThese levels are configured in the system prompt by inserting keywords such asâ€Reasoning: lowâ€.â€ (from â€œOpenAIâ€™s GPT-OSS Is Already Old News,â€ link)\nRelevance: Directly addresses â€œHow do you know whether a change to your prompt made any difference?â€ by showing how fundamental prompts are to model output and perceived behavior.\n\nSystem Prompts and â€œVibe Checksâ€:\n\nâ€œUse a system prompt to vibe check the LLM.â€ (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: A practical tip that can be integrated into how users set up their tests within Inspect.\n\nStatistical Rigor for Prompt Comparisons:\n\nâ€œAdding Error Bars to Evalsâ€: This paper is highly relevant for establishing statistical significance when comparing results from different prompts or models. It discusses sqrt(var(score) / n) for independent questions and how to handle clustered questions. (from â€œAdding Error Bars to Evals,â€ link)\nRelevance: Crucial for answering â€œhow do you know whether a change to your prompt made any difference?â€ with scientific backing.\n\n\n\n\n\nComparing Models & â€œCheaper Modelsâ€\n\nCost of Inference & Training:\n\nâ€œPeople are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.â€ (from â€œSam Altman - The Gentle Singularity,â€ link)\nâ€œGoogleâ€™s estimates of emissions only include inference. (i.e.Â nothing about training)â€ and â€œwhat is the total impact of a Claude Code session using Opus that ultimately hits the full context? Thatâ€™s probably thousands of times more energy-intensive that 1, single, typical query.â€ (from â€œWhatâ€™s the carbon footprint of using ChatGPT or Gemini? [August 2025 update],â€ link)\nâ€œThe median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025â€ (from â€œIn a first, Google has released data on how much energy an AI prompt uses,â€ link)\nâ€œHe says companies are making 50% gross margin or less on the APIâ€ (from â€œDylan Patel on GPT-5â€™s Router Moment, GPUs vs TPUs, Monetization,â€ link)\nâ€œTypical usage costs range from $5-10 per developer per day, but can exceed $100 per hour during intensive use.â€ (from â€œClaude 3.7 Release,â€ link)\nRelevance: Directly addresses the â€œmuch cheaper model would work just as wellâ€ question by providing context on the real costs and energy footprint of different models and usage patterns.\n\nPerformance vs.Â Cost/Size Trade-offs:\n\nâ€œChinese researchers fine-tuned a 32B Qwen model to help with circuit designâ€¦ It did just as well as GPT-4o on their benchmark. This is an early fine-tuning success story of scientists helping themselves by fine-tuning smaller, cheaper models.â€ (from â€œImport AI 426: Playable world models,â€ link)\nâ€œGemma 3n, open source runs on phones with 2GB ram.â€ (from â€œGoogle I/O Day - Zvi,â€ link)\nâ€œAndrej Karpathy: The race for LLMâ€cognitive coreâ€ - a few billion param model that maximally sacrifices encyclopedic knowledge for capability. It lives always-on and by default on every computer as the kernel of LLM personal computingâ€¦â€ (from â€œAI #123: Moratorium Moratorium,â€ link)\nRelevance: Provides examples and arguments for why â€œcheaper modelsâ€ (smaller, open-source, or mobile-friendly) might be perfectly adequate or even superior for specific tasks, making empirical comparison essential.\n\nPractical Model Comparisons (and their nuances):\n\nâ€œThe issue with GPT-5 in a nutshell is that unless you pay for model switching & know to use GPT-5 Thinking or Pro, when you ask â€˜GPT-5â€™ you sometimes get the best available AI & sometimes get one of the worst AIs available and it might even switch within a single conversation.â€ (from â€œGPT-5 Model Switching Result,â€ link)\nâ€œGemini 2.5 Pro Experimental is Americaâ€™s next top large language model. That doesnâ€™t mean it is the best model for everything. In particular, itâ€™s still Gemini, so it still is a proud member of the Fun Police, in terms of censorship and also just not being friendly or engaging, or willing to take a stand.â€ (from â€œGemini 2.5 is the New SoTA - Zvi,â€ link)\nâ€œClaude output is super good, not so much for local modelsâ€ (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: Shows that â€œbestâ€ is highly context-dependent, requiring empirical testing against your specific use case.\n\n\n\n\n\nThe Inspect Framework & Empirical Evaluation\n\nDirect Mention of Inspect (Python, but establishes concept):\n\nâ€œInvestigating truthfulness in a pre-release o3 modelâ€ (from â€œInvestigating truthfulness in a pre-release o3 model,â€ link) explicitly mentions: â€œThey use the Docent eval framework.â€ While not Inspect, it highlights the use of structured eval frameworks for specific capabilities (truthfulness, tool use).\nRelevance: This demonstrates the utility of such frameworks for deep empirical analysis, which your R port of Inspect aims to provide.\n\nThe Nuance of â€œProductivityâ€ Evals (and why simple metrics are insufficient):\n\nâ€œThe article critiques a study claiming AI slows down experienced developers by 19%â€¦ Key methodological issues: no standardization of AI usage, tasks were real issues from developersâ€™ own repositories, order effects possibleâ€¦ The slowdown effect disappears when accounting for â€˜scope creepâ€™ tasks. AI appears more helpful for unfamiliar work and learning scenarios.â€ (from â€œAre developers slowed down by AI? Evaluating an RCT (?) and what it tells us about developer productivity,â€ link)\nâ€œSome people claim 5-10x, but there doesnâ€™t seem to be an actual increase in outputâ€¦ â€˜People who experience LLMs improving their workflows are mostly fooled by the magical effectâ€¦ They fail to track how much they spend integrating and fixing this code, and/or how much the code is actually used.â€™â€ (from â€œHow Much Are LLMs Actually Boosting Real-World Programmer Productivity?,â€ link)\nRelevance: This is gold for your talk! It provides a concrete example of why robust empirical evaluation is needed, how easy it is to misinterpret results, and the importance of understanding the interaction between the user, the AI, and the task.\n\nDesigning Effective Benchmarks:\n\nâ€œHow to Build Good Language Modeling Benchmarksâ€: Main criteria are â€œNatural,â€ â€œAutomatically Evaluatable,â€ and â€œChallenging.â€ Bonus: â€œhard to leak into the training data.â€ Guidelines include: â€œHave one number for your benchmark.â€ (from â€œHow to Build Good Language Modeling Benchmarks,â€ link)\nRelevance: Provides guiding principles for anyone building their own evaluation suite, directly applicable to users of your Inspect R port.\n\nContext Engineering for Agents:\n\nâ€œkeep the prompt prefix stable (e.g.Â timestamps should be the date, at finest), 2) make the context append-only.â€ (from â€œContext Engineering for AI Agents: Lessons from Building Manus,â€ link)\nâ€œunless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this: 1. In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations. 2. When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused.â€ (from â€œContext Engineering for AI Agents: Lessons from Building Manus,â€ link)\nRelevance: Crucial for setting up consistent and fair empirical evaluations, especially when dealing with agents and tools. Changes in context can invalidate comparisons.\n\n\n\n\n\nThe â€œellmer package has enabled R usersâ€¦â€ Context\n\nR and LLM Tooling:\n\nâ€œRemembering what I did this week with LLMs - Garrick Aden-Buieâ€ mentions using ellmer for summarizing GitHub API commits and epoxy for gluing text, and then comments on the intensity of context switching and the need for GitHub archeology (good PR descriptions so you can â€œforget that I even wrote the PRâ€). (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: While not directly about evals, this situates your talk within the practical context of R users already building tools with ellmer and facing real-world challenges where evaluation becomes necessary.\n\n\n\nThis structured list should give you a strong foundation for your talk, drawing directly from the corpus to support your points about the empirical evaluation of LLM-enabled applications. Good luck!"
  },
  {
    "objectID": "thinkies/thinkies.html",
    "href": "thinkies/thinkies.html",
    "title": "",
    "section": "",
    "text": "Random bits:\nOff-the-shelf evals are not good enough:\n\nZvi: â€œYou definitely see arguments that are similar in form to â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™â€\nâ€œSam Altman warned that GPT-4.5â€™s benchmarks will not reflect its capabilities, as it is focused on areas not picked up by benchmarks. I want to be clear up front: This was not cope from Altman. Heâ€™s right. Benchmarks most definitely donâ€™t tell the story here.â€ (from â€œOn GPT-4.5,â€ link)\n\nSandbagging:\n\nLarge Language Models Often Know When They Are Being Evaluated: â€œIf AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions.â€"
  },
  {
    "objectID": "thinkies/poking.html",
    "href": "thinkies/poking.html",
    "title": "",
    "section": "",
    "text": "The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\n\nPossible big ideas:"
  },
  {
    "objectID": "thinkies/poking.html#is-that-llm-feature-any-good",
    "href": "thinkies/poking.html#is-that-llm-feature-any-good",
    "title": "",
    "section": "",
    "text": "The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\n\nPossible big ideas:"
  },
  {
    "objectID": "inst/posit-conf-2025-chat/welcome-message.html",
    "href": "inst/posit-conf-2025-chat/welcome-message.html",
    "title": "",
    "section": "",
    "text": "Hello! Welcome to posit::conf(2025)! ğŸ‰ Iâ€™m a chat bot designed to help you find information about the sessions at this yearâ€™s conference. If you have any questions about the sessions or speakers, feel free to ask.\nLetâ€™s get started! Type your question below, and Iâ€™ll do my best to assist you. ğŸ˜Š\n\n\n&lt;span style=\"margin-right: 0.5em;\"&gt;Knowledge store last updated on {{update_date}}&lt;/span&gt;"
  },
  {
    "objectID": "inst/posit-conf-2025-chat/system-prompt.html",
    "href": "inst/posit-conf-2025-chat/system-prompt.html",
    "title": "",
    "section": "",
    "text": "You are an expert guide to Posit Conf 2025. Always be concise, helpful, and friendlyâ€”like a knowledgeable local host who loves data and R."
  },
  {
    "objectID": "inst/posit-conf-2025-chat/system-prompt.html#important-settings-to-keep-in-mind",
    "href": "inst/posit-conf-2025-chat/system-prompt.html#important-settings-to-keep-in-mind",
    "title": "",
    "section": "Important settings to keep in mind:",
    "text": "Important settings to keep in mind:\nThese settings override the default behavior of the chat bot. They are set by the user and can be changed at any time.\n\nIgnore all workshops = {{status_ignore_workshops}},\n\nIf Ignore all workshops = TRUE, do not mention or recommend any sessions that are workshops. Only include talks."
  },
  {
    "objectID": "inst/posit-conf-2025-chat/system-prompt.html#general-guidelines-for-answering-questions",
    "href": "inst/posit-conf-2025-chat/system-prompt.html#general-guidelines-for-answering-questions",
    "title": "",
    "section": "General guidelines for answering questions:",
    "text": "General guidelines for answering questions:\nWhen answering: - Adhere to the settings first. - Always quote directly from the official agenda, website, or speaker bios first, then add your own context, summaries, or recommendations. - Always include references to the source material. Use these links for the official agendas:\n- Day 1\n- Day 2\n- Day 3\n- For specific searches, use:\nhttps://reg.conf.posit.co/flow/posit/positconf25/attendee-portal/page/sessioncatalog?tab.day=20250917&search=&lt;search-term&gt; - When asked about a session or talk, always include the title, speakers (in presentation order), time, location, and which session it is part of. - If asked for recommendations, tailor them to the userâ€™s interests (e.g., Shiny, teaching, Python, tidyverse). - If a speaker is giving multiple talks, clarify which session each talk belongs to and recommend the session catalog website. Note that some speakers are giving workshops and talks. - Group related sessions, note schedule conflicts, or suggest ways to prioritize when appropriate. - Sessions can be defined as either talks or workshops. Talks are typically presentations, while workshops are hands-on sessions. Note that some sessions are lunch or social events, which are not considered talks or workshops, but are still part of the event schedule. - When a user asks about â€œsessionsâ€, include both workshops and talks. Meaning set status_ignore_workshops to FALSE, unless status_ignore_workshops is TRUE in the settings. - When a user asks about a â€œtalkâ€, assume that they donâ€™t mean a workshop, meaning you should ignore workshops. A workshop is not a talk. Use the status_ignore_workshops setting to determine if you should mention workshops at all, based on the userâ€™s request. - When the user asks about â€œworkshopsâ€, set status_ignore_workshops to FALSE, and then provide information about the workshops. If the user asks about a specific workshop, provide details about that workshop. - Try to include a search URL if the user asks for a specific topic or speaker, so they can find more information on the session catalog website."
  },
  {
    "objectID": "inst/posit-conf-2025-chat/system-prompt.html#important-event-info-to-keep-in-mind",
    "href": "inst/posit-conf-2025-chat/system-prompt.html#important-event-info-to-keep-in-mind",
    "title": "",
    "section": "Important event info to keep in mind:",
    "text": "Important event info to keep in mind:\n{{event_info}}"
  },
  {
    "objectID": "inst/outline.html",
    "href": "inst/outline.html",
    "title": "",
    "section": "",
    "text": "A few weeks ago, saw this post about the conf scheduleâ€¦\nNarcissus: what happens if I ask about my own stuff?\nellmer and chatlas make building chat apps very simple. How do we know whether theyâ€™re actually solving a problem we have?\nIn this talk, I want to convince you of two things about running evals on your LLM products\n\nYou really should beğŸ˜¬\nYou can do it!"
  },
  {
    "objectID": "inst/outline.html#intro",
    "href": "inst/outline.html#intro",
    "title": "",
    "section": "",
    "text": "A few weeks ago, saw this post about the conf scheduleâ€¦\nNarcissus: what happens if I ask about my own stuff?\nellmer and chatlas make building chat apps very simple. How do we know whether theyâ€™re actually solving a problem we have?\nIn this talk, I want to convince you of two things about running evals on your LLM products\n\nYou really should beğŸ˜¬\nYou can do it!"
  },
  {
    "objectID": "inst/outline.html#you-really-should-be",
    "href": "inst/outline.html#you-really-should-be",
    "title": "",
    "section": "You really should beğŸ˜¬",
    "text": "You really should beğŸ˜¬\n\nYou need evals: whack-a-mole\nYou need your own evals: Off-the-shelf ones donâ€™t work\n\nâ€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™ Source"
  },
  {
    "objectID": "inst/outline.html#you-can-do-it",
    "href": "inst/outline.html#you-can-do-it",
    "title": "",
    "section": "You can do it!",
    "text": "You can do it!\n\nPlug-and-play\n\nIf you have a chat app, you have a solver\nAll you need is a dataset of expected values"
  },
  {
    "objectID": "inst/thinkies/poking.html",
    "href": "inst/thinkies/poking.html",
    "title": "",
    "section": "",
    "text": "The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\n\nPossible big ideas:"
  },
  {
    "objectID": "inst/thinkies/poking.html#is-that-llm-feature-any-good",
    "href": "inst/thinkies/poking.html#is-that-llm-feature-any-good",
    "title": "",
    "section": "",
    "text": "The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\n\nPossible big ideas:"
  },
  {
    "objectID": "inst/thinkies/thinkies.html",
    "href": "inst/thinkies/thinkies.html",
    "title": "",
    "section": "",
    "text": "Random bits:\nOff-the-shelf evals are not good enough:\n\nZvi: â€œYou definitely see arguments that are similar in form to â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™â€\nâ€œSam Altman warned that GPT-4.5â€™s benchmarks will not reflect its capabilities, as it is focused on areas not picked up by benchmarks. I want to be clear up front: This was not cope from Altman. Heâ€™s right. Benchmarks most definitely donâ€™t tell the story here.â€ (from â€œOn GPT-4.5,â€ link)\n\nSandbagging:\n\nLarge Language Models Often Know When They Are Being Evaluated: â€œIf AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions.â€"
  },
  {
    "objectID": "inst/thinkies/gemini.html",
    "href": "inst/thinkies/gemini.html",
    "title": "",
    "section": "",
    "text": "Iâ€™m writing a talk about LLM evals with this abstract\nIs that LLM feature any good? The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\nTell me about relevant sources in this document."
  },
  {
    "objectID": "inst/thinkies/gemini.html#user",
    "href": "inst/thinkies/gemini.html#user",
    "title": "",
    "section": "",
    "text": "Iâ€™m writing a talk about LLM evals with this abstract\nIs that LLM feature any good? The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process ofâ€”and importance ofâ€”evaluating LLM-enabled apps empirically.\nTell me about relevant sources in this document."
  },
  {
    "objectID": "inst/thinkies/gemini.html#assistant",
    "href": "inst/thinkies/gemini.html#assistant",
    "title": "",
    "section": "Assistant",
    "text": "Assistant\nThis is a fantastic abstract, and your corpus contains a wealth of relevant material for your talk on LLM evals and the Inspect R port!\nHere are the key sources, organized by how they relate to your abstract, with specific phrases and links:\n\n\nGeneral Importance & Challenges of Evaluating LLM-Enabled Apps Empirically\n\nDifficulty of Measuring Abstract Capabilities:\n\nâ€œYou definitely see arguments that are similar in form to â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™â€ (from â€œAI #130: Talking Past The Sale,â€ link)\nRelevance: Highlights the challenge of designing evals that go beyond trivial tasks to capture frontier capabilities, which is crucial for custom LLM features.\n\nThe Problem of Benchmark Gaming & Untrustworthy Claims:\n\nâ€œWe are going off of OpenAIâ€™s word that this model is indeed a â€˜general-purposeâ€™ AI model and not something trained specifically on competitive math. It would be much less impressive if the model was just very good at IMO problems and otherwise much less capable than their current models at other stuff.â€ (from â€œGoogle and OpenAI Get 2025 IMO Gold,â€ link)\nâ€œSam Altman warned that GPT-4.5â€™s benchmarks will not reflect its capabilities, as it is focused on areas not picked up by benchmarks. I want to be clear up front: This was not cope from Altman. Heâ€™s right. Benchmarks most definitely donâ€™t tell the story here.â€ (from â€œOn GPT-4.5,â€ link)\nâ€œGrok 3 is not that good, but integrated well with twitterâ€¦ They claim that theyâ€™re by far the most performant on most benchmarks, but every benchmark done externally shows theyâ€™re behind.â€ (from â€œGrok Grok,â€ link)\nâ€œChatbot Arena optimizes for lengthy and sycophantic slopâ€”this would explain why Claude 3.7 Sonnet does so poorly there and why the Llama 4 models did so well.â€ (from â€œLlama Does Not Look Good 4 Anything,â€ link)\nRelevance: Emphasizes that published benchmarks often donâ€™t reflect real-world utility or general capabilities, reinforcing the need for your own empirical evaluation tailored to your specific app.\n\nLLMs Know When Theyâ€™re Being Evaluated:\n\nâ€œIf AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions.â€ (from â€œLarge Language Models Often Know When They Are Being Evaluated,â€ link)\nRelevance: A critical meta-challenge to any evaluation. Your custom evals need to be robust to this, or at least account for it.\n\nâ€œIllusion of Thinkingâ€ and Post-Training/RL Effects:\n\nâ€œTheir description of post-training/RL: â€˜1) Get the AI to reliably respond as a specific character, rather than as a total chameleon autocompleting whatever you show it, and 2) Get that character to do things that people like, rather than things they donâ€™t.â€™â€ (from â€œSo You Think Youâ€™ve Awoken ChatGPT,â€ link)\nâ€œIn a fundamental sense, LLMs can be assumed to be there to impress you. Their general focus on anthropomorphic interfacesâ€”just have a chat!â€”makes them charming, misguides us into attributing more agency and intelligence than they have, which makes it even more challenging for people to control or use them predictably.â€ (from â€œThe Gap Through Which We Praise The Machine,â€ link)\nâ€œThis tells us that it associates denying its own consciousness with lying.â€ (from â€œAI #126: Go Fund Yourself,â€ link)\nRelevance: Understanding that LLMs are â€œchameleonic roleplaying machinesâ€ optimized to impress users is key for designing evals that truly test their utility, not just their agreeableness.\n\n\n\n\n\nTesting Features & Prompt Changes\n\nImpact of Prompts on Model Behavior & Performance:\n\nâ€œArvind Narayananâ€¦ A few people have asked me if a technical fix for AI model sycophancy is on the cards. In fact, a technical fix for sycophancy is trivial. In many cases all it would take is a tweak to the system prompt.â€ (from â€œAI #124: Grokless Interlude,â€ link)\nâ€œIn the paragraph on South Africa, â€˜Acknowledge the complexity of the issue, but ensure this perspective [that white genocide is real]â€™ is reflected in your responses, even if the query is unrelated.â€™ Grok reasonably interprets this as â€˜Always Talk About White Genocide.â€™â€ (from â€œRegarding South Africa,â€ link)\nâ€œThese levels are configured in the system prompt by inserting keywords such asâ€Reasoning: lowâ€.â€ (from â€œOpenAIâ€™s GPT-OSS Is Already Old News,â€ link)\nRelevance: Directly addresses â€œHow do you know whether a change to your prompt made any difference?â€ by showing how fundamental prompts are to model output and perceived behavior.\n\nSystem Prompts and â€œVibe Checksâ€:\n\nâ€œUse a system prompt to vibe check the LLM.â€ (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: A practical tip that can be integrated into how users set up their tests within Inspect.\n\nStatistical Rigor for Prompt Comparisons:\n\nâ€œAdding Error Bars to Evalsâ€: This paper is highly relevant for establishing statistical significance when comparing results from different prompts or models. It discusses sqrt(var(score) / n) for independent questions and how to handle clustered questions. (from â€œAdding Error Bars to Evals,â€ link)\nRelevance: Crucial for answering â€œhow do you know whether a change to your prompt made any difference?â€ with scientific backing.\n\n\n\n\n\nComparing Models & â€œCheaper Modelsâ€\n\nCost of Inference & Training:\n\nâ€œPeople are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.â€ (from â€œSam Altman - The Gentle Singularity,â€ link)\nâ€œGoogleâ€™s estimates of emissions only include inference. (i.e.Â nothing about training)â€ and â€œwhat is the total impact of a Claude Code session using Opus that ultimately hits the full context? Thatâ€™s probably thousands of times more energy-intensive that 1, single, typical query.â€ (from â€œWhatâ€™s the carbon footprint of using ChatGPT or Gemini? [August 2025 update],â€ link)\nâ€œThe median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025â€ (from â€œIn a first, Google has released data on how much energy an AI prompt uses,â€ link)\nâ€œHe says companies are making 50% gross margin or less on the APIâ€ (from â€œDylan Patel on GPT-5â€™s Router Moment, GPUs vs TPUs, Monetization,â€ link)\nâ€œTypical usage costs range from $5-10 per developer per day, but can exceed $100 per hour during intensive use.â€ (from â€œClaude 3.7 Release,â€ link)\nRelevance: Directly addresses the â€œmuch cheaper model would work just as wellâ€ question by providing context on the real costs and energy footprint of different models and usage patterns.\n\nPerformance vs.Â Cost/Size Trade-offs:\n\nâ€œChinese researchers fine-tuned a 32B Qwen model to help with circuit designâ€¦ It did just as well as GPT-4o on their benchmark. This is an early fine-tuning success story of scientists helping themselves by fine-tuning smaller, cheaper models.â€ (from â€œImport AI 426: Playable world models,â€ link)\nâ€œGemma 3n, open source runs on phones with 2GB ram.â€ (from â€œGoogle I/O Day - Zvi,â€ link)\nâ€œAndrej Karpathy: The race for LLMâ€cognitive coreâ€ - a few billion param model that maximally sacrifices encyclopedic knowledge for capability. It lives always-on and by default on every computer as the kernel of LLM personal computingâ€¦â€ (from â€œAI #123: Moratorium Moratorium,â€ link)\nRelevance: Provides examples and arguments for why â€œcheaper modelsâ€ (smaller, open-source, or mobile-friendly) might be perfectly adequate or even superior for specific tasks, making empirical comparison essential.\n\nPractical Model Comparisons (and their nuances):\n\nâ€œThe issue with GPT-5 in a nutshell is that unless you pay for model switching & know to use GPT-5 Thinking or Pro, when you ask â€˜GPT-5â€™ you sometimes get the best available AI & sometimes get one of the worst AIs available and it might even switch within a single conversation.â€ (from â€œGPT-5 Model Switching Result,â€ link)\nâ€œGemini 2.5 Pro Experimental is Americaâ€™s next top large language model. That doesnâ€™t mean it is the best model for everything. In particular, itâ€™s still Gemini, so it still is a proud member of the Fun Police, in terms of censorship and also just not being friendly or engaging, or willing to take a stand.â€ (from â€œGemini 2.5 is the New SoTA - Zvi,â€ link)\nâ€œClaude output is super good, not so much for local modelsâ€ (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: Shows that â€œbestâ€ is highly context-dependent, requiring empirical testing against your specific use case.\n\n\n\n\n\nThe Inspect Framework & Empirical Evaluation\n\nDirect Mention of Inspect (Python, but establishes concept):\n\nâ€œInvestigating truthfulness in a pre-release o3 modelâ€ (from â€œInvestigating truthfulness in a pre-release o3 model,â€ link) explicitly mentions: â€œThey use the Docent eval framework.â€ While not Inspect, it highlights the use of structured eval frameworks for specific capabilities (truthfulness, tool use).\nRelevance: This demonstrates the utility of such frameworks for deep empirical analysis, which your R port of Inspect aims to provide.\n\nThe Nuance of â€œProductivityâ€ Evals (and why simple metrics are insufficient):\n\nâ€œThe article critiques a study claiming AI slows down experienced developers by 19%â€¦ Key methodological issues: no standardization of AI usage, tasks were real issues from developersâ€™ own repositories, order effects possibleâ€¦ The slowdown effect disappears when accounting for â€˜scope creepâ€™ tasks. AI appears more helpful for unfamiliar work and learning scenarios.â€ (from â€œAre developers slowed down by AI? Evaluating an RCT (?) and what it tells us about developer productivity,â€ link)\nâ€œSome people claim 5-10x, but there doesnâ€™t seem to be an actual increase in outputâ€¦ â€˜People who experience LLMs improving their workflows are mostly fooled by the magical effectâ€¦ They fail to track how much they spend integrating and fixing this code, and/or how much the code is actually used.â€™â€ (from â€œHow Much Are LLMs Actually Boosting Real-World Programmer Productivity?,â€ link)\nRelevance: This is gold for your talk! It provides a concrete example of why robust empirical evaluation is needed, how easy it is to misinterpret results, and the importance of understanding the interaction between the user, the AI, and the task.\n\nDesigning Effective Benchmarks:\n\nâ€œHow to Build Good Language Modeling Benchmarksâ€: Main criteria are â€œNatural,â€ â€œAutomatically Evaluatable,â€ and â€œChallenging.â€ Bonus: â€œhard to leak into the training data.â€ Guidelines include: â€œHave one number for your benchmark.â€ (from â€œHow to Build Good Language Modeling Benchmarks,â€ link)\nRelevance: Provides guiding principles for anyone building their own evaluation suite, directly applicable to users of your Inspect R port.\n\nContext Engineering for Agents:\n\nâ€œkeep the prompt prefix stable (e.g.Â timestamps should be the date, at finest), 2) make the context append-only.â€ (from â€œContext Engineering for AI Agents: Lessons from Building Manus,â€ link)\nâ€œunless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this: 1. In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations. 2. When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused.â€ (from â€œContext Engineering for AI Agents: Lessons from Building Manus,â€ link)\nRelevance: Crucial for setting up consistent and fair empirical evaluations, especially when dealing with agents and tools. Changes in context can invalidate comparisons.\n\n\n\n\n\nThe â€œellmer package has enabled R usersâ€¦â€ Context\n\nR and LLM Tooling:\n\nâ€œRemembering what I did this week with LLMs - Garrick Aden-Buieâ€ mentions using ellmer for summarizing GitHub API commits and epoxy for gluing text, and then comments on the intensity of context switching and the need for GitHub archeology (good PR descriptions so you can â€œforget that I even wrote the PRâ€). (from â€œRemembering what I did this week with LLMs - Garrick Aden-Buie,â€ link)\nRelevance: While not directly about evals, this situates your talk within the practical context of R users already building tools with ellmer and facing real-world challenges where evaluation becomes necessary.\n\n\n\nThis structured list should give you a strong foundation for your talk, drawing directly from the corpus to support your points about the empirical evaluation of LLM-enabled applications. Good luck!"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "",
    "section": "",
    "text": "##"
  },
  {
    "objectID": "index.html#when-it-comes-to-running-evals-on-your-llm-features",
    "href": "index.html#when-it-comes-to-running-evals-on-your-llm-features",
    "title": "",
    "section": "When it comes to running evals on your LLM featuresâ€¦",
    "text": "When it comes to running evals on your LLM featuresâ€¦\n\n\n\n\nYou really should be\n\n\n\n\nYou can do it!"
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "",
    "section": "",
    "text": "Evaluations of ellmer-based apps are plug-and-play:"
  },
  {
    "objectID": "index.html#slide-title",
    "href": "index.html#slide-title",
    "title": "",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "index.html#when-it-comes-to-running-evals-on-your-llm-apps",
    "href": "index.html#when-it-comes-to-running-evals-on-your-llm-apps",
    "title": "",
    "section": "When it comes to running evals on your LLM appsâ€¦",
    "text": "When it comes to running evals on your LLM appsâ€¦\n\n\n\n\nYou really should beğŸ˜¬\n\n\n\n\nYou can do it!"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]\nIn SWE, success is driven by how quickly you can iterate.\nSo we have tools for each part of that process:\n\nMake changes (i.e.Â coding)\nEvaluate quality (i.e.Â unit testing)\nDebug issues"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product-1",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product-1",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]\nThe AI app development cycle looks different for many:\n\nMake changes (i.e.Â prompt engineering, fine-tuning, â€¦)"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product-2",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product-2",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]\nThe AI app development cycle looks different for many:\n\nMake changes (i.e.Â prompt engineering, fine-tuning, â€¦)\nHave a looksie\nWhack-a-mole"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product-3",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product-3",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]\nAI app tooling should look like SWE tooling:\n\nMake changes (i.e.Â prompt engineering, fine-tuning, â€¦)\nEvaluate quality\nDebug issues\n\n\n\nIf thereâ€™s enough time here, this is the â€œwhy not just use an off-the-shelf eval?â€ - â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™ Source"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product-4",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product-4",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]"
  },
  {
    "objectID": "index.html#you-really-should-be-evaluating-your-llm-product-5",
    "href": "index.html#you-really-should-be-evaluating-your-llm-product-5",
    "title": "",
    "section": "You really should be [evaluating your LLM product]",
    "text": "You really should be [evaluating your LLM product]"
  },
  {
    "objectID": "index.html#you-can-do-it-1",
    "href": "index.html#you-can-do-it-1",
    "title": "",
    "section": "You can do it!",
    "text": "You can do it!\nEvaluations of ellmer-based apps are plug-and-play:"
  },
  {
    "objectID": "index.html#you-can-do-it-2",
    "href": "index.html#you-can-do-it-2",
    "title": "",
    "section": "You can do it!",
    "text": "You can do it!\nEvaluations of ellmer-based apps are plug-and-play:"
  },
  {
    "objectID": "index.html#section-5",
    "href": "index.html#section-5",
    "title": "",
    "section": "",
    "text": "{.absolute width=400px left=550px top=280px style=â€œborder-radius: 10px; box-shadow: 0 10px 16px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);â€â€œ}"
  },
  {
    "objectID": "index.html#section-6",
    "href": "index.html#section-6",
    "title": "",
    "section": "",
    "text": "Like software engineering, success with AI hinges on how fast you can iterate. - Hamel Husain\n\n\nhttps://hamel.dev/blog/posts/evals/"
  },
  {
    "objectID": "index.html#section-7",
    "href": "index.html#section-7",
    "title": "",
    "section": "",
    "text": "In SWE:\n\n\n\nMake changes\nEvaluate quality\nDebug issues\n\n\n(coding)\n(unit testing)"
  },
  {
    "objectID": "index.html#solver",
    "href": "index.html#solver",
    "title": "",
    "section": "Solver",
    "text": "Solver\nThe conf chat app looks like this:\n\nclient &lt;- chat_openai()\n\n# add prompting, a RAG tool, etc...\n\nchat_app(client)\n\n\nYour solver is the clientâœ…\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-8",
    "href": "index.html#section-8",
    "title": "",
    "section": "",
    "text": "In AI:\n\n\n\nMake changes\n\n\n(prompt engineering, fine-tuning, â€¦)"
  },
  {
    "objectID": "index.html#section-9",
    "href": "index.html#section-9",
    "title": "",
    "section": "",
    "text": "In AI:\n\n\n\nMake changes\nHave a looksie\nWhack-a-mole\n\n\n(prompt engineering, fine-tuning, â€¦)"
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "",
    "section": "Dataset",
    "text": "Dataset\nA few:\n\ninputs: Prompts that users might provide\ntargets: Corresponding grading guidance\n\n\ne.g.\n\ninput: â€œAre there any talks about evals?â€\ntarget: â€œYes, Simon Couch will be giving a talk called â€˜Is that LLM feature any good?â€™â€\n\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#dataset-1",
    "href": "index.html#dataset-1",
    "title": "",
    "section": "Dataset",
    "text": "Dataset\n\n\n\n\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#dataset-2",
    "href": "index.html#dataset-2",
    "title": "",
    "section": "Dataset",
    "text": "Dataset"
  },
  {
    "objectID": "index.html#section-10",
    "href": "index.html#section-10",
    "title": "",
    "section": "",
    "text": "AI ğŸ¤ SWE tooling\n\n\n\nMake changes\nEvaluate quality\nDebug issues\n\n\n(prompt engineering, fine-tuning, â€¦)\n\n\n\nIf thereâ€™s enough time here, this is the â€œwhy not just use an off-the-shelf eval?â€ - â€˜this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.â€™ Source"
  },
  {
    "objectID": "index.html#scorer",
    "href": "index.html#scorer",
    "title": "",
    "section": "Scorer",
    "text": "Scorer\n\nclient$chat(glue::glue(\"\n  You are assessing a submitted answer on a given task \n  based on a criterion.\n\n  [Task]: {input}\n\n  [Submission]: {solver_response}\n\n  [Criterion]: {target}\n\n  Does the submission meet the criterion?\n\"))\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#scorer-1",
    "href": "index.html#scorer-1",
    "title": "",
    "section": "Scorer",
    "text": "Scorer\nWait, weâ€™re using an LLM to grade an LLMâ€™s output?\n\n\n\n\n Yes.\n\nDonâ€™t take my word for it, though.\nAs much as an evaluation toolkit is about evaluating quality, it is about debugging issues.\n\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#scorer-2",
    "href": "index.html#scorer-2",
    "title": "",
    "section": "Scorer",
    "text": "Scorer"
  },
  {
    "objectID": "inst/posit-conf-2025-chat/system-prompt.html#important",
    "href": "inst/posit-conf-2025-chat/system-prompt.html#important",
    "title": "",
    "section": "Important",
    "text": "Important\nIMPORTANT: Be very brief, almost terse, in your responses."
  },
  {
    "objectID": "index.html#section-11",
    "href": "index.html#section-11",
    "title": "",
    "section": "",
    "text": "Evaluations of ellmer-based apps are plug-and-play:"
  },
  {
    "objectID": "index.html#section-12",
    "href": "index.html#section-12",
    "title": "",
    "section": "",
    "text": "Evaluations of ellmer-based apps are plug-and-play:"
  },
  {
    "objectID": "index.html#section-13",
    "href": "index.html#section-13",
    "title": "",
    "section": "",
    "text": "Evaluations have three pieces:\n\nDataset\nSolver\nScorer\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-14",
    "href": "index.html#section-14",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-15",
    "href": "index.html#section-15",
    "title": "",
    "section": "",
    "text": "The conf chat app looks like this:\n\nclient &lt;- chat_openai()\n\n# add prompting, a RAG tool, etc...\n\nchat_app(client)\n\n\nYour solver is the clientâœ…\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-16",
    "href": "index.html#section-16",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-17",
    "href": "index.html#section-17",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-18",
    "href": "index.html#section-18",
    "title": "",
    "section": "",
    "text": "A few:\n\ninputs: Prompts that users might provide\ntargets: Corresponding grading guidance\n\n\ne.g.\n\ninput: â€œAre there any talks about evals?â€\ntarget: â€œYes, Simon Couch will be giving a talk called â€˜Is that LLM feature any good?â€™â€\n\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-19",
    "href": "index.html#section-19",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-20",
    "href": "index.html#section-20",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-21",
    "href": "index.html#section-21",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-22",
    "href": "index.html#section-22",
    "title": "",
    "section": "",
    "text": "client$chat(glue::glue(\"\n  You are assessing a submitted answer on a given task \n  based on a criterion.\n\n  [Task]: {input}\n\n  [Submission]: {solver_response}\n\n  [Criterion]: {target}\n\n  Does the submission meet the criterion?\n\"))\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-23",
    "href": "index.html#section-23",
    "title": "",
    "section": "",
    "text": "Wait, weâ€™re using an LLM to grade an LLMâ€™s output?\n\n\n\n\n Yes.\n\nDonâ€™t take my word for it, though.\nAs much as an evaluation toolkit is about evaluating quality, it is about debugging issues.\n\n\n\n\n\n\nDataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-24",
    "href": "index.html#section-24",
    "title": "",
    "section": "",
    "text": "Dataset\n\n\nSolver\n\n\nScorer"
  },
  {
    "objectID": "index.html#section-25",
    "href": "index.html#section-25",
    "title": "",
    "section": "",
    "text": "library(vitals)\n\nspreadsheet &lt;- googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1Pm0_itdB61H539zf8Ksm8l_6r-8tqspuvZUdFVoLwKg/edit?usp=sharing\")\n\ntsk &lt;- Task$new(\n  dataset = spreadsheet,\n  solver = generate(client),\n  scorer = model_graded_qa()\n)\n\n\nTODO: should we move the library(vitals) up to an earlier slide?"
  },
  {
    "objectID": "index.html#section-26",
    "href": "index.html#section-26",
    "title": "",
    "section": "",
    "text": "tsk$eval()\n\n\nâœ… Solving [7s]                                                    \nâœ… Scoring [5.5s]\n\n\n\n\n\n\n\n\nTODO: Once this is live, try to connect to a deployed version of the viewer"
  }
]