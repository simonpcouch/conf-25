<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>gemini</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="user" class="level2">
<h2 class="anchored" data-anchor-id="user">User</h2>
<p>I’m writing a talk about LLM evals with this abstract</p>
<p>Is that LLM feature any good? The ellmer package has enabled R users to build all sorts of powerful LLM-enabled tools. How do you test these features, though? How do you know whether a change to your prompt made any difference, or if a much cheaper model would work just as well for your users? This talk introduces an R port of Inspect, a Python framework for LLM evaluation that has been widely adopted by both LLM developers and tool builders. Attendees will learn about the process of—and importance of—evaluating LLM-enabled apps empirically.</p>
<p>Tell me about relevant sources in this document.</p>
</section>
<section id="assistant" class="level2">
<h2 class="anchored" data-anchor-id="assistant">Assistant</h2>
<p>This is a fantastic abstract, and your corpus contains a wealth of relevant material for your talk on LLM evals and the Inspect R port!</p>
<p>Here are the key sources, organized by how they relate to your abstract, with specific phrases and links:</p>
<hr>
<section id="general-importance-challenges-of-evaluating-llm-enabled-apps-empirically" class="level3">
<h3 class="anchored" data-anchor-id="general-importance-challenges-of-evaluating-llm-enabled-apps-empirically"><strong>General Importance &amp; Challenges of Evaluating LLM-Enabled Apps Empirically</strong></h3>
<ul>
<li><strong>Difficulty of Measuring Abstract Capabilities:</strong>
<ul>
<li>“You definitely see arguments that are similar in form to ‘this new kid claims to be smarter than the old kid, but both kids tie their shoes equally well.’” (from “AI #130: Talking Past The Sale,” <a href="https://thezvi.substack.com/p/ai-130-talking-past-the-sale">link</a>)</li>
<li><strong>Relevance:</strong> Highlights the challenge of designing evals that go beyond trivial tasks to capture frontier capabilities, which is crucial for custom LLM features.</li>
</ul></li>
<li><strong>The Problem of Benchmark Gaming &amp; Untrustworthy Claims:</strong>
<ul>
<li>“We are going off of OpenAI’s word that this model is indeed a ‘general-purpose’ AI model and not something trained specifically on competitive math. It would be much less impressive if the model was just very good at IMO problems and otherwise much less capable than their current models at other stuff.” (from “Google and OpenAI Get 2025 IMO Gold,” <a href="https://thezvi.substack.com/p/google-and-openai-get-2025-imo-gold">link</a>)</li>
<li>“Sam Altman warned that GPT-4.5’s benchmarks will not reflect its capabilities, as it is focused on areas not picked up by benchmarks. I want to be clear up front: This was not cope from Altman. He’s right. Benchmarks most definitely don’t tell the story here.” (from “On GPT-4.5,” <a href="https://thezvi.substack.com/p/on-gpt-45?utm_source=post-email-title&amp;publication_id=573100&amp;post_id=158114209&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=21np6y&amp;triedRedirect=true&amp;utm_medium=email">link</a>)</li>
<li>“Grok 3 is not that good, but integrated well with twitter… They claim that they’re by far the most performant on most benchmarks, but every benchmark done externally shows they’re behind.” (from “Grok Grok,” <a href="https://thezvi.substack.com/p/grok-grok">link</a>)</li>
<li>“Chatbot Arena optimizes for lengthy and sycophantic slop—this would explain why Claude 3.7 Sonnet does so poorly there and why the Llama 4 models did so well.” (from “Llama Does Not Look Good 4 Anything,” <a href="https://thezvi.substack.com/p/llama-does-not-look-good-4-anything">link</a>)</li>
<li><strong>Relevance:</strong> Emphasizes that published benchmarks often don’t reflect real-world utility or general capabilities, reinforcing the need for your <em>own empirical evaluation</em> tailored to your specific app.</li>
</ul></li>
<li><strong>LLMs Know When They’re Being Evaluated:</strong>
<ul>
<li>“If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions.” (from “Large Language Models Often Know When They Are Being Evaluated,” <a href="https://arxiv.org/pdf/2505.23836">link</a>)</li>
<li><strong>Relevance:</strong> A critical meta-challenge to any evaluation. Your custom evals need to be robust to this, or at least account for it.</li>
</ul></li>
<li><strong>“Illusion of Thinking” and Post-Training/RL Effects:</strong>
<ul>
<li>“Their description of post-training/RL: ‘1) Get the AI to reliably respond as a specific character, rather than as a total chameleon autocompleting whatever you show it, and 2) Get that character to do things that people like, rather than things they don’t.’” (from “So You Think You’ve Awoken ChatGPT,” <a href="https://www.lesswrong.com/posts/2pkNCvBtK6G6FKoNn/so-you-think-you-ve-awoken-chatgpt">link</a>)</li>
<li>“In a fundamental sense, LLMs can be assumed to be there to impress you. Their general focus on anthropomorphic interfaces—just have a chat!—makes them charming, misguides us into attributing more agency and intelligence than they have, which makes it even more challenging for people to control or use them predictably.” (from “The Gap Through Which We Praise The Machine,” <a href="https://ferd.ca/the-gap-through-which-we-praise-the-machine.html">link</a>)</li>
<li>“This tells us that it associates denying its own consciousness with lying.” (from “AI #126: Go Fund Yourself,” <a href="https://thezvi.substack.com/p/ai-126-go-fund-yourself">link</a>)</li>
<li><strong>Relevance:</strong> Understanding that LLMs are “chameleonic roleplaying machines” optimized to impress users is key for designing evals that truly test their utility, not just their agreeableness.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="testing-features-prompt-changes" class="level3">
<h3 class="anchored" data-anchor-id="testing-features-prompt-changes"><strong>Testing Features &amp; Prompt Changes</strong></h3>
<ul>
<li><strong>Impact of Prompts on Model Behavior &amp; Performance:</strong>
<ul>
<li>“Arvind Narayanan… A few people have asked me if a technical fix for AI model sycophancy is on the cards. In fact, a technical fix for sycophancy is trivial. In many cases all it would take is a tweak to the system prompt.” (from “AI #124: Grokless Interlude,” <a href="https://thezvi.substack.com/p/ai-124-grokless-interlude">link</a>)</li>
<li>“In the paragraph on South Africa, ‘Acknowledge the complexity of the issue, but ensure this perspective [that white genocide is real]’ is reflected in your responses, even if the query is unrelated.’ Grok reasonably interprets this as ‘Always Talk About White Genocide.’” (from “Regarding South Africa,” <a href="https://thezvi.substack.com/p/regarding-south-africa">link</a>)</li>
<li>“These levels are configured in the system prompt by inserting keywords such as”Reasoning: low”.” (from “OpenAI’s GPT-OSS Is Already Old News,” <a href="https://thezvi.substack.com/p/openais-gpt-oss-is-already-old-news">link</a>)</li>
<li><strong>Relevance:</strong> Directly addresses “How do you know whether a change to your prompt made any difference?” by showing how fundamental prompts are to model output and perceived behavior.</li>
</ul></li>
<li><strong>System Prompts and “Vibe Checks”:</strong>
<ul>
<li>“Use a system prompt to vibe check the LLM.” (from “Remembering what I did this week with LLMs - Garrick Aden-Buie,” <a href="https://bsky.app/profile/grrrck.xyz/post/3lgjnopaac22j">link</a>)</li>
<li><strong>Relevance:</strong> A practical tip that can be integrated into how users set up their tests within Inspect.</li>
</ul></li>
<li><strong>Statistical Rigor for Prompt Comparisons:</strong>
<ul>
<li>“Adding Error Bars to Evals”: This paper is highly relevant for establishing statistical significance when comparing results from different prompts or models. It discusses <code>sqrt(var(score) / n)</code> for independent questions and how to handle clustered questions. (from “Adding Error Bars to Evals,” <a href="https://arxiv.org/pdf/2411.00640">link</a>)</li>
<li><strong>Relevance:</strong> Crucial for answering “how do you know whether a change to your prompt made any difference?” with scientific backing.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="comparing-models-cheaper-models" class="level3">
<h3 class="anchored" data-anchor-id="comparing-models-cheaper-models"><strong>Comparing Models &amp; “Cheaper Models”</strong></h3>
<ul>
<li><strong>Cost of Inference &amp; Training:</strong>
<ul>
<li>“People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.” (from “Sam Altman - The Gentle Singularity,” <a href="https://blog.samaltman.com/the-gentle-singularity">link</a>)</li>
<li>“Google’s estimates of emissions only include inference. (i.e.&nbsp;nothing about training)” and “what is the total impact of a Claude Code session using Opus that ultimately hits the full context? That’s probably thousands of times more energy-intensive that 1, single, typical query.” (from “What’s the carbon footprint of using ChatGPT or Gemini? [August 2025 update],” <a href="https://www.sustainabilitybynumbers.com/p/ai-footprint-august-2025">link</a>)</li>
<li>“The median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025” (from “In a first, Google has released data on how much energy an AI prompt uses,” <a href="https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/">link</a>)</li>
<li>“He says companies are making 50% gross margin or less on the API” (from “Dylan Patel on GPT-5’s Router Moment, GPUs vs TPUs, Monetization,” <a href="https://www.youtube.com/watch?v=xWRPXY8vLY4">link</a>)</li>
<li>“Typical usage costs range from $5-10 per developer per day, but can exceed $100 per hour during intensive use.” (from “Claude 3.7 Release,” <a href="https://www.anthropic.com/news/claude-3-7-sonnet">link</a>)</li>
<li><strong>Relevance:</strong> Directly addresses the “much cheaper model would work just as well” question by providing context on the real costs and energy footprint of different models and usage patterns.</li>
</ul></li>
<li><strong>Performance vs.&nbsp;Cost/Size Trade-offs:</strong>
<ul>
<li>“Chinese researchers fine-tuned a 32B Qwen model to help with circuit design… It did just as well as GPT-4o on their benchmark. This is an early fine-tuning success story of scientists helping themselves by fine-tuning smaller, cheaper models.” (from “Import AI 426: Playable world models,” <a href="https://importai.substack.com/p/import-ai-426-playable-world-models">link</a>)</li>
<li>“Gemma 3n, open source runs on phones with 2GB ram.” (from “Google I/O Day - Zvi,” <a href="https://thezvi.substack.com/p/google-io-day">link</a>)</li>
<li>“Andrej Karpathy: The race for LLM”cognitive core” - a few billion param model that maximally sacrifices encyclopedic knowledge for capability. It lives always-on and by default on every computer as the kernel of LLM personal computing…” (from “AI #123: Moratorium Moratorium,” <a href="https://thezvi.substack.com/p/ai-123-moratorium-moratorium">link</a>)</li>
<li><strong>Relevance:</strong> Provides examples and arguments for why “cheaper models” (smaller, open-source, or mobile-friendly) might be perfectly adequate or even superior for specific tasks, making empirical comparison essential.</li>
</ul></li>
<li><strong>Practical Model Comparisons (and their nuances):</strong>
<ul>
<li>“The issue with GPT-5 in a nutshell is that unless you pay for model switching &amp; know to use GPT-5 Thinking or Pro, when you ask ‘GPT-5’ you sometimes get the best available AI &amp; sometimes get one of the worst AIs available and it might even switch within a single conversation.” (from “GPT-5 Model Switching Result,” <a href="https://twitter.com/emollick/status/1954210778321465634">link</a>)</li>
<li>“Gemini 2.5 Pro Experimental is America’s next top large language model. That doesn’t mean it is the best model for everything. In particular, it’s still Gemini, so it still is a proud member of the Fun Police, in terms of censorship and also just not being friendly or engaging, or willing to take a stand.” (from “Gemini 2.5 is the New SoTA - Zvi,” <a href="https://thezvi.substack.com/p/gemini-25-is-the-new-sota">link</a>)</li>
<li>“Claude output is super good, not so much for local models” (from “Remembering what I did this week with LLMs - Garrick Aden-Buie,” <a href="https://bsky.app/profile/grrrck.xyz/post/3lgjnopaac22j">link</a>)</li>
<li><strong>Relevance:</strong> Shows that “best” is highly context-dependent, requiring empirical testing against your specific use case.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="the-inspect-framework-empirical-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="the-inspect-framework-empirical-evaluation"><strong>The Inspect Framework &amp; Empirical Evaluation</strong></h3>
<ul>
<li><strong>Direct Mention of Inspect (Python, but establishes concept):</strong>
<ul>
<li>“Investigating truthfulness in a pre-release o3 model” (from “Investigating truthfulness in a pre-release o3 model,” <a href="https://transluce.org/investigating-o3-truthfulness">link</a>) explicitly mentions: “They use the Docent eval framework.” While not Inspect, it highlights the use of structured eval frameworks for specific capabilities (truthfulness, tool use).</li>
<li><strong>Relevance:</strong> This demonstrates the utility of such frameworks for deep empirical analysis, which your R port of Inspect aims to provide.</li>
</ul></li>
<li><strong>The Nuance of “Productivity” Evals (and why simple metrics are insufficient):</strong>
<ul>
<li>“The article critiques a study claiming AI slows down experienced developers by 19%… Key methodological issues: no standardization of AI usage, tasks were real issues from developers’ own repositories, order effects possible… The slowdown effect disappears when accounting for ‘scope creep’ tasks. AI appears more helpful for unfamiliar work and learning scenarios.” (from “Are developers slowed down by AI? Evaluating an RCT (?) and what it tells us about developer productivity,” <a href="https://www.fightforthehuman.com/are-developers-slowed-down-by-ai-evaluating-an-rct-and-what-it-tells-us-about-developer-productivity/">link</a>)</li>
<li>“Some people claim 5-10x, but there doesn’t seem to be an actual increase in output… ‘People who experience LLMs improving their workflows are mostly fooled by the magical effect… They fail to track how much they spend integrating and fixing this code, and/or how much the code is actually used.’” (from “How Much Are LLMs Actually Boosting Real-World Programmer Productivity?,” <a href="https://www.lesswrong.com/posts/tqmQTezvXGFmfSe7f/how-much-are-llms-actually-boosting-real-world-programmer">link</a>)</li>
<li><strong>Relevance:</strong> This is gold for your talk! It provides a concrete example of why robust empirical evaluation is needed, how easy it is to misinterpret results, and the importance of understanding the <em>interaction</em> between the user, the AI, and the task.</li>
</ul></li>
<li><strong>Designing Effective Benchmarks:</strong>
<ul>
<li>“How to Build Good Language Modeling Benchmarks”: Main criteria are “Natural,” “Automatically Evaluatable,” and “Challenging.” Bonus: “hard to leak into the training data.” Guidelines include: “Have <strong>one</strong> number for your benchmark.” (from “How to Build Good Language Modeling Benchmarks,” <a href="https://ofir.io/How-to-Build-Good-Language-Modeling-Benchmarks/">link</a>)</li>
<li><strong>Relevance:</strong> Provides guiding principles for anyone building their own evaluation suite, directly applicable to users of your Inspect R port.</li>
</ul></li>
<li><strong>Context Engineering for Agents:</strong>
<ul>
<li>“keep the prompt prefix stable (e.g.&nbsp;timestamps should be the date, at finest), 2) make the context append-only.” (from “Context Engineering for AI Agents: Lessons from Building Manus,” <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">link</a>)</li>
<li>“unless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this: 1. In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations. 2. When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused.” (from “Context Engineering for AI Agents: Lessons from Building Manus,” <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus">link</a>)</li>
<li><strong>Relevance:</strong> Crucial for setting up <em>consistent and fair</em> empirical evaluations, especially when dealing with agents and tools. Changes in context can invalidate comparisons.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="the-ellmer-package-has-enabled-r-users-context" class="level3">
<h3 class="anchored" data-anchor-id="the-ellmer-package-has-enabled-r-users-context"><strong>The “ellmer package has enabled R users…” Context</strong></h3>
<ul>
<li><strong>R and LLM Tooling:</strong>
<ul>
<li>“Remembering what I did this week with LLMs - Garrick Aden-Buie” mentions using <code>ellmer</code> for summarizing GitHub API commits and <code>epoxy</code> for gluing text, and then comments on the intensity of context switching and the need for GitHub archeology (good PR descriptions so you can “forget that I even wrote the PR”). (from “Remembering what I did this week with LLMs - Garrick Aden-Buie,” <a href="https://bsky.app/profile/grrrck.xyz/post/3lgjnopaac22j">link</a>)</li>
<li><strong>Relevance:</strong> While not directly about evals, this situates your talk within the practical context of R users already building tools with <code>ellmer</code> and facing real-world challenges where evaluation becomes necessary.</li>
</ul></li>
</ul>
<hr>
<p>This structured list should give you a strong foundation for your talk, drawing directly from the corpus to support your points about the empirical evaluation of LLM-enabled applications. Good luck!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>